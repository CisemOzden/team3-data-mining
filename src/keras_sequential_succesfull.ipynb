{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import Image, display\n",
    "import tensorflow\n",
    "os.environ['PATH'] += ';C:\\\\Program Files\\\\Graphviz\\\\bin'\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# !pip install talos\n",
    "import talos\n",
    "\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_variables = dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "TRAINING_FILES = [\"../data/original/training_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\difference_preprocessed_training_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\ole_RemoveOutliers_and_preprocessing_pipe_training_dataset.csv\"]\n",
    "EVALUATION_FILES = [\"../data/original/evaluation_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\difference_preprocessed_evaluation_dataset.csv\"\n",
    ", \"..\\data\\preprocessed\\ole_RemoveOutliers_and_preprocessing_pipe_evaluation_dataset.csv\"]\n",
    "\n",
    "FILE_N = 2\n",
    "INPUT_TRAINING_FILE = TRAINING_FILES[FILE_N] \n",
    "INPUT_EVALUATION_FILE = EVALUATION_FILES[FILE_N]\n",
    "\n",
    "\n",
    "# Output parameters\n",
    "METHOD_NAME = \"keras_tensorflow_sequential\"\n",
    "TIMESTAMP = time.strftime(\"%d_%m_%Y-%H_%M_%S\")\n",
    "OUTPUT_MODEL_FOLDER = f\"../data/models/keras_models\"\n",
    "OUTPUT_MODEL = f\"../data/models/{METHOD_NAME}_keras_model_{TIMESTAMP}.pkl\"\n",
    "OUTPUT_RESULTS = f\"../data/results/{METHOD_NAME}_model_{TIMESTAMP}.txt\"\n",
    "OUTPUT_MODEL_ARCHITECTURE_RESULTS = f\"../img/{METHOD_NAME}_model_{TIMESTAMP}.png\"\n",
    "\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_SCORING = \"accuracy\"\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 5\n",
    "\n",
    "# Cross validation parameters\n",
    "CROSS_VALIDATION_CV = 5\n",
    "\n",
    "# Cost parameters\n",
    "PUNISHMENT_FOR_FALSE_BANKRUPT_PREDICTION = 100\n",
    "\n",
    "# Other constants\n",
    "LABELS = [\"Operational\", \"Bankrupt\"]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Hyper parameter alternatives\n",
    "HYPER_PARAMETER_K_FEATURES = list(range(30, 90, 10))\n",
    "HYPER_PARAMETER_SCORE_FUNC = [chi2, f_classif]\n",
    "HYPER_PARAMETER_HIDDEN_LAYER_SIZES = [(100,)]\n",
    "HYPER_PARAMETER_ALPHA = list(10.0 ** -numpy.arange(1, 7))\n",
    "HYPER_PARAMETER_CLASIFIER_OPTIMIZERS = ['rmsprop','adam','adagrad']\n",
    "HYPER_PARAMETER_EPOCHS = [10,20,40,60]\n",
    "HYPER_PARAMETER_DROPUT = [0,0.1,0.2,0.3]\n",
    "HYPER_PARAMETER_LAYERS_N = list(range(20,90,10))\n",
    "HYPER_PARAMETER_BATCH_NUMBER = list(range(20,160,30))\n",
    "HYPER_PARAMETER_KERNEL_INITIALIZER = ['glorot_uniform','normal','uniform']\n",
    "HYPER_PARAMETER_CLASIFIER_METRICS = ['accuracy']\n",
    "\n",
    "# Hyper parameter optimization parameters\n",
    "HYPER_PARAMETER_OPTIMIZATION_BETA = 2\n",
    "HYPER_PARAMETER_OPTIMIZATION_CV = 5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5.012000e+03</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5012.0</td>\n",
       "      <td>5012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.034916</td>\n",
       "      <td>0.505281</td>\n",
       "      <td>0.558882</td>\n",
       "      <td>0.553725</td>\n",
       "      <td>0.607314</td>\n",
       "      <td>0.607293</td>\n",
       "      <td>0.998697</td>\n",
       "      <td>0.797135</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>0.303650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807912</td>\n",
       "      <td>1.572626e+07</td>\n",
       "      <td>0.623718</td>\n",
       "      <td>0.607312</td>\n",
       "      <td>0.840565</td>\n",
       "      <td>0.280603</td>\n",
       "      <td>0.027672</td>\n",
       "      <td>0.565481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.183586</td>\n",
       "      <td>0.058682</td>\n",
       "      <td>0.062425</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.015162</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.015840</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037703</td>\n",
       "      <td>3.320811e+08</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.018144</td>\n",
       "      <td>0.009452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432653</td>\n",
       "      <td>0.432653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224792</td>\n",
       "      <td>1.015558e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432654</td>\n",
       "      <td>0.344652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477490</td>\n",
       "      <td>0.536402</td>\n",
       "      <td>0.528080</td>\n",
       "      <td>0.600203</td>\n",
       "      <td>0.600180</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>0.797387</td>\n",
       "      <td>0.809313</td>\n",
       "      <td>0.303466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797028</td>\n",
       "      <td>9.614392e-04</td>\n",
       "      <td>0.623627</td>\n",
       "      <td>0.600204</td>\n",
       "      <td>0.840141</td>\n",
       "      <td>0.277343</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503096</td>\n",
       "      <td>0.560074</td>\n",
       "      <td>0.552492</td>\n",
       "      <td>0.605457</td>\n",
       "      <td>0.605443</td>\n",
       "      <td>0.999022</td>\n",
       "      <td>0.797460</td>\n",
       "      <td>0.809373</td>\n",
       "      <td>0.303523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810546</td>\n",
       "      <td>2.188898e-03</td>\n",
       "      <td>0.623858</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>0.841223</td>\n",
       "      <td>0.279089</td>\n",
       "      <td>0.026815</td>\n",
       "      <td>0.565282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534600</td>\n",
       "      <td>0.587999</td>\n",
       "      <td>0.582981</td>\n",
       "      <td>0.612939</td>\n",
       "      <td>0.612889</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.797567</td>\n",
       "      <td>0.809460</td>\n",
       "      <td>0.303580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825835</td>\n",
       "      <td>5.622987e-03</td>\n",
       "      <td>0.624123</td>\n",
       "      <td>0.612939</td>\n",
       "      <td>0.842379</td>\n",
       "      <td>0.281773</td>\n",
       "      <td>0.026925</td>\n",
       "      <td>0.565777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.046495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981315</td>\n",
       "      <td>9.820000e+09</td>\n",
       "      <td>0.956387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.736985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.095511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "count  5012.000000                                        5012.000000          \n",
       "mean      0.034916                                           0.505281          \n",
       "std       0.183586                                           0.058682          \n",
       "min       0.000000                                           0.000000          \n",
       "25%       0.000000                                           0.477490          \n",
       "50%       0.000000                                           0.503096          \n",
       "75%       0.000000                                           0.534600          \n",
       "max       1.000000                                           0.971530          \n",
       "\n",
       "        ROA(A) before interest and % after tax  \\\n",
       "count                              5012.000000   \n",
       "mean                                  0.558882   \n",
       "std                                   0.062425   \n",
       "min                                   0.006923   \n",
       "25%                                   0.536402   \n",
       "50%                                   0.560074   \n",
       "75%                                   0.587999   \n",
       "max                                   1.000000   \n",
       "\n",
       "        ROA(B) before interest and depreciation after tax  \\\n",
       "count                                        5012.000000    \n",
       "mean                                            0.553725    \n",
       "std                                             0.059237    \n",
       "min                                             0.000000    \n",
       "25%                                             0.528080    \n",
       "50%                                             0.552492    \n",
       "75%                                             0.582981    \n",
       "max                                             1.000000    \n",
       "\n",
       "        Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "count              5012.000000                   5012.000000   \n",
       "mean                  0.607314                      0.607293   \n",
       "std                   0.013633                      0.013615   \n",
       "min                   0.432653                      0.432653   \n",
       "25%                   0.600203                      0.600180   \n",
       "50%                   0.605457                      0.605443   \n",
       "75%                   0.612939                      0.612889   \n",
       "max                   1.000000                      1.000000   \n",
       "\n",
       "        Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "count             5012.000000                 5012.000000   \n",
       "mean                 0.998697                    0.797135   \n",
       "std                  0.015162                    0.014980   \n",
       "min                  0.000000                    0.000000   \n",
       "25%                  0.998971                    0.797387   \n",
       "50%                  0.999022                    0.797460   \n",
       "75%                  0.999089                    0.797567   \n",
       "max                  0.999778                    1.000000   \n",
       "\n",
       "        After-tax net Interest Rate  \\\n",
       "count                   5012.000000   \n",
       "mean                       0.809022   \n",
       "std                        0.015840   \n",
       "min                        0.000000   \n",
       "25%                        0.809313   \n",
       "50%                        0.809373   \n",
       "75%                        0.809460   \n",
       "max                        1.000000   \n",
       "\n",
       "        Non-industry income and expenditure/revenue  ...  \\\n",
       "count                                   5012.000000  ...   \n",
       "mean                                       0.303650  ...   \n",
       "std                                        0.013002  ...   \n",
       "min                                        0.000000  ...   \n",
       "25%                                        0.303466  ...   \n",
       "50%                                        0.303523  ...   \n",
       "75%                                        0.303580  ...   \n",
       "max                                        1.000000  ...   \n",
       "\n",
       "        Net Income to Total Assets   Total assets to GNP price  \\\n",
       "count                  5012.000000                5.012000e+03   \n",
       "mean                      0.807912                1.572626e+07   \n",
       "std                       0.037703                3.320811e+08   \n",
       "min                       0.224792                1.015558e-04   \n",
       "25%                       0.797028                9.614392e-04   \n",
       "50%                       0.810546                2.188898e-03   \n",
       "75%                       0.825835                5.622987e-03   \n",
       "max                       0.981315                9.820000e+09   \n",
       "\n",
       "        No-credit Interval   Gross Profit to Sales  \\\n",
       "count          5012.000000             5012.000000   \n",
       "mean              0.623718                0.607312   \n",
       "std               0.011597                0.013633   \n",
       "min               0.000000                0.432654   \n",
       "25%               0.623627                0.600204   \n",
       "50%               0.623858                0.605454   \n",
       "75%               0.624123                0.612939   \n",
       "max               0.956387                1.000000   \n",
       "\n",
       "        Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "count                          5012.000000           5012.000000   \n",
       "mean                              0.840565              0.280603   \n",
       "std                               0.009603              0.010669   \n",
       "min                               0.344652              0.000000   \n",
       "25%                               0.840141              0.277343   \n",
       "50%                               0.841223              0.279089   \n",
       "75%                               0.842379              0.281773   \n",
       "max                               1.000000              0.652347   \n",
       "\n",
       "        Degree of Financial Leverage (DFL)  \\\n",
       "count                          5012.000000   \n",
       "mean                              0.027672   \n",
       "std                               0.018144   \n",
       "min                               0.000000   \n",
       "25%                               0.026791   \n",
       "50%                               0.026815   \n",
       "75%                               0.026925   \n",
       "max                               1.000000   \n",
       "\n",
       "        Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "count                                        5012.000000              5012.0   \n",
       "mean                                            0.565481                 1.0   \n",
       "std                                             0.009452                 0.0   \n",
       "min                                             0.172065                 1.0   \n",
       "25%                                             0.565158                 1.0   \n",
       "50%                                             0.565282                 1.0   \n",
       "75%                                             0.565777                 1.0   \n",
       "max                                             0.736985                 1.0   \n",
       "\n",
       "        Equity to Liability  \n",
       "count           5012.000000  \n",
       "mean               0.037410  \n",
       "std                0.018318  \n",
       "min                0.000000  \n",
       "25%                0.023851  \n",
       "50%                0.032154  \n",
       "75%                0.046495  \n",
       "max                0.095511  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = pd.read_csv(INPUT_TRAINING_FILE, engine=\"python\", delimiter=\",\")\n",
    "training_features = training_dataset.loc[:, training_dataset.columns != \"Bankrupt?\"]\n",
    "training_targets = training_dataset[\"Bankrupt?\"]\n",
    "\n",
    "evaluation_dataset = pd.read_csv(INPUT_EVALUATION_FILE, engine=\"python\", delimiter=\",\")\n",
    "evaluation_features = evaluation_dataset.loc[:, evaluation_dataset.columns != \"Bankrupt?\"]\n",
    "evaluation_targets = evaluation_dataset[\"Bankrupt?\"]\n",
    "\n",
    "training_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with creating more complex functionall model for Keras Tensorflow classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.engine.functional.Functional'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def keras_model_for_sklearn(optimizer='adam',#'adagrad',\n",
    "                    loss = 'binary_crossentropy', \n",
    "                    kernel_initializer='glorot_uniform', \n",
    "                    dropout=0.2, layers_n  = 95,\n",
    "                    n_activator = 'relu',\n",
    "                    neurons = 213 ,\n",
    "                    n_out = 1,\n",
    "                    f_activator = 'relu',\n",
    "                    batch_norm = True,\n",
    "                    metrics = ['accuracy', 'MeanSquaredError', 'AUC', \"RootMeanSquaredError\"]\n",
    "                  ):\n",
    "        \n",
    "    \n",
    "    input =tensorflow.keras.layers.Input(shape=(layers_n,))\n",
    "    output1 =tensorflow.keras.layers.Dense(layers_n, input_shape=(layers_n,), activation=n_activator)(input)\n",
    "    if batch_norm:\n",
    "        output1 =tensorflow.keras.layers.BatchNormalization(axis=1, name='dense_relu' + '2a')(output1)\n",
    "    output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "    dense_name_base = \"dense\"\n",
    "    bn_name_base = \"dense_additionall\"\n",
    "    shortcut1 =tensorflow.keras.layers.Dense(layers_n, activation=n_activator, kernel_initializer='he_normal',\n",
    "                                name=dense_name_base + '1')(output1)\n",
    "    output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "\n",
    "    output1 =tensorflow.keras.layers.Dropout(dropout)(output1)\n",
    "    output =tensorflow.keras.layers.Dense(n_out, activation=f_activator, name='fc1000')(output1)\n",
    "    \n",
    "    # Final model\n",
    "    model =tensorflow.keras.models.Model(inputs=input, outputs=output)\n",
    "\n",
    "    # Plot model information\n",
    "    plot_= False\n",
    "    if plot_:\n",
    "        plot_model(model, to_file=OUTPUT_MODEL_ARCHITECTURE_RESULTS, show_shapes=True, show_layer_names=True)\n",
    "        im = cv2.imread(OUTPUT_MODEL_ARCHITECTURE_RESULTS, 0)\n",
    "        model.summary()\n",
    "        display(Image(filename=OUTPUT_MODEL_ARCHITECTURE_RESULTS))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "print(type(keras_model_for_sklearn()))# <class 'keras.engine.functional.Functional'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dense, Dropout,     BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, fbeta_score, make_scorer\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score, accuracy_score, precision_score, precision_recall_curve, average_precision_score\n",
    "from keras.layers import Input, Dense, Add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIUAAAA8CAIAAADHfe7WAAAABmJLR0QA/wD/AP+gvaeTAAAD/UlEQVR4nO2cP1LrMBDGpTfvDlRwBdrkEHIVhskNXDOU9nABQ8uMU1I4/KmSGlq7NKVLQyVX9gn8im+eRrETxwlJWMj+KksrrSV90krRGGRd14Ihw5/vbgCzAOtBC9aDFqwHLf7aiTiO7+7uvqspx8lwOLy6ujLJhfXx+fn58vJy8CYdL0mSxHFs5/xtF3p+fj5Ue46di4uLRg7vH7RgPWjBetCC9aAF60EL1oMWrActWA9asB60YD1owXrQgvWgBetBC9aDFr9cD9/3fd/vKFAUxXQ6dRznYE3q5rfpUVWVlLJ/+Zubm/F4PJ/P+xQuisL3fSmllHI6nfY0bUZt8fj42Mj5ccxms0270B6HpWit4zjGcxRFQoggCNaauhmNRqPRaKExduKn61GWpVJqT3qYEW/X6jB109Zjy3h1e3srpZxMJkVR2PGhKAqYHMd5e3uzqyBMO46TJMl8Pkct+R+UaSSXOrQjPvw4jvPx8YFZicgDJ429oaqqyWQCk+/7RVFs1OXBYGCeq6oSQniet9a0MbY4PddHEAR5ntd1XZYlXox8rbVSKoqiuq5fX1+FEGmawuR5nlJKa21MqKW1tpuR57mdXOoQK0AIgVmJKq7rtuemKYmk67pCCK11R5U+5HmOXmdZ1t+0lN3EK/QKzxhQPCN02sU8z6tb2tSLQ9AYDju5ymFHlQ6T53lLNdhIDzNjRGuT6DCtYjd6YKJFUVSWpZ1v5mNj/aH8wlv7jeAqh9vpAfI8D4Jgaz1AmqZYB2EY9je12Y0eWZaZkbInwqqOtfN7jmBPh/31CMNQKZVl2Rf1qOu64aSnqcEuz1dpmmLiG0nQiHbo/KIeax329Iboh53v63p0V9xaj23OV1LKqqrOz8/v7+/TNL2+vkZ+GIZCiIeHB5wxcDSCYEKI9/f3TV+0yuF2jMdjIcTZ2dnWHmzQJGjc37QeW5z++7nneZhoCMfIN4clA8pg/SqlkMRPNvMiLDIsAvPxJDbepQ5NJnavsiyRxBEDgVRrHQSBKWmb8jw38URr3SjTgVKqcbDE4aLb1M0uz1eY9e0zBnYz13XRPmCCWxiGZgRNFYzUbDZD36IoMqPTdtiYTI1kmqaYLg0tGyactewT0dqOm2mEXtu/ATtM3bT1kHaXnp6eLi8vG53cB/jFd4AXEQff79ofTP+2+8SfzjfoYS4qNr2xOAaW/L3Bvjk5OTEPpEJW90X9YZr6DXqQ0sCGQsN4/6AF60EL1oMWrActWA9asB60YD1owXrQgvWgBetBC9aDFqwHLVgPWiy5323/0xlmTyRJYn9rKhrr4/T0dDQaHbZJR81gMBgOh3aOpHDpzxh4/6AF60EL1oMWrAct/gHOKU4zqHrIVAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEIVID~1\\AppData\\Local\\Temp/ipykernel_12296/140970796.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m                             )\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\UNI\\Jupyter\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_tf = True    \n",
    "# create a function that returns a model, taking as parameters things you\n",
    "# want to verify using cross-valdiation and model selection\n",
    "def create_model(optimizer='adam', kernel_initializer='normal', dropout=0.2, layers_n = 64, metrics=\"accuracy\", batchn = True, hidden_layer_sizes = [20,90], plot_tf = False, x,y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers_n,activation='relu',kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(dropout))\n",
    "    if batchn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(1,activation='relu',kernel_initializer=kernel_initializer))\n",
    "    if plot_tf:\n",
    "        plot_model(model, to_file=OUTPUT_MODEL_ARCHITECTURE_RESULTS, show_shapes=True, show_layer_names=True)\n",
    "        im = cv2.imread(OUTPUT_MODEL_ARCHITECTURE_RESULTS, 0)\n",
    "        model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=[metrics])\n",
    "        # model.summary()\n",
    "        display(Image(filename=OUTPUT_MODEL_ARCHITECTURE_RESULTS))\n",
    "        plot_tf = False\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=[metrics])\n",
    "    return model\n",
    "\n",
    "param_grid = {\n",
    "    \"selection__k\": HYPER_PARAMETER_K_FEATURES,\n",
    "    \"selection__score_func\": HYPER_PARAMETER_SCORE_FUNC,\n",
    "    'clf__optimizer':HYPER_PARAMETER_CLASIFIER_OPTIMIZERS,\n",
    "    'clf__epochs':HYPER_PARAMETER_EPOCHS,\n",
    "    'clf__dropout':HYPER_PARAMETER_DROPUT,\n",
    "    'clf__layers_n' : HYPER_PARAMETER_LAYERS_N,\n",
    "    'clf__batchn' : HYPER_PARAMETER_BATCH_NUMBER,\n",
    "    'clf__kernel_initializer':HYPER_PARAMETER_KERNEL_INITIALIZER,\n",
    "    'clf__metrics':HYPER_PARAMETER_CLASIFIER_METRICS,\n",
    "}\n",
    "\n",
    "# Best alternative:\n",
    "param_grid = {\n",
    "    'clf__optimizer':['adam'],\n",
    "    'clf__epochs':[10],\n",
    "    'clf__dropout':[0.2],\n",
    "    'clf__layers_n' : [64],\n",
    "    'clf__kernel_initializer':['uniform'],\n",
    "    'clf__metrics':['accuracy'],\n",
    "    \"selection__k\": [30],\n",
    "    \"selection__score_func\": [chi2],\n",
    "}\n",
    "\n",
    "create_model(optimizer=param_grid['clf__optimizer'][-1],\n",
    "            dropout=param_grid['clf__dropout'][-1],\n",
    "            layers_n=param_grid['clf__layers_n'][-1],\n",
    "            kernel_initializer=param_grid['clf__kernel_initializer'][-1],\n",
    "            metrics = param_grid['clf__metrics'][-1],\n",
    "plot_tf=True)\n",
    "\n",
    "# Using keras regresor for our custom model\n",
    "clf = KerasRegressor(build_fn=create_model,verbose=3)\n",
    "\n",
    "# Definng standart scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Do feature selection with SelectKBest.\n",
    "feature_selection = SelectKBest(f_classif,  k=64)  \n",
    "\n",
    "# Oversample data with SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"minority\", random_state=RANDOM_SEED)\n",
    "X_train, y_train = training_features,training_targets\n",
    "X_train, y_train = smote.fit_resample(training_features,training_targets)\n",
    "# Min max scaler definition \n",
    "min_max_scaler =  MinMaxScaler()\n",
    "\n",
    "# Feature selection \n",
    "feature_selection = SelectKBest()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess',scaler),\n",
    "    (\"min_max_scaler\", min_max_scaler),\n",
    "    (\"selection\", feature_selection),\n",
    "    ('clf',clf)\n",
    "])\n",
    "\n",
    "# Define custom fbeta scorer function that put emphasis on recall\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    confidence = 0\n",
    "    y_pred = (np.where(y_pred > confidence, 1, 0))    \n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    return fbeta_score(y_true, y_pred, beta=HYPER_PARAMETER_OPTIMIZATION_BETA)\n",
    "\n",
    "grid = GridSearchCV(pipeline, \n",
    "                            cv=5,\n",
    "                            param_grid=param_grid\n",
    "                            , verbose = 3 \n",
    "                            , scoring=make_scorer(custom_scorer)\n",
    "                            )\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Acc. Score = 0.572992, with parameters {'clf__dropout': 0.2, 'clf__epochs': 10, 'clf__kernel_initializer': 'uniform', 'clf__layers_n': 64, 'clf__metrics': 'accuracy', 'clf__optimizer': 'adam', 'selection__k': 30, 'selection__score_func': <function chi2 at 0x0000017B1577CB80>}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model: Acc. Score = %f, with parameters %s\" % (grid.best_score_, grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence 0.0\n",
      "Balanced training. acc: 0.6495761835848667\n",
      "Balanced eval. acc: 0.640530303030303\n",
      "Confidence 0.02\n",
      "Balanced training. acc: 0.6580525118875336\n",
      "Balanced eval. acc: 0.6446969696969697\n",
      "Confidence 0.04\n",
      "Balanced training. acc: 0.6674591689063469\n",
      "Balanced eval. acc: 0.6526515151515151\n",
      "Confidence 0.06\n",
      "Balanced training. acc: 0.6789332230721521\n",
      "Balanced eval. acc: 0.665530303030303\n",
      "Confidence 0.08\n",
      "Balanced training. acc: 0.6875501343808146\n",
      "Balanced eval. acc: 0.6738636363636363\n",
      "Confidence 0.1\n",
      "Balanced training. acc: 0.6975770105437255\n",
      "Balanced eval. acc: 0.6806818181818182\n",
      "Confidence 0.12\n",
      "Balanced training. acc: 0.7044366342774446\n",
      "Balanced eval. acc: 0.6863636363636363\n",
      "Confidence 0.14\n",
      "Balanced training. acc: 0.7129501757287575\n",
      "Balanced eval. acc: 0.6962121212121212\n",
      "Confidence 0.16\n",
      "Balanced training. acc: 0.7239073806078148\n",
      "Balanced eval. acc: 0.7079545454545454\n",
      "Confidence 0.18\n",
      "Balanced training. acc: 0.7337647302046723\n",
      "Balanced eval. acc: 0.7170454545454545\n",
      "Confidence 0.2\n",
      "Balanced training. acc: 0.7451354145131279\n",
      "Balanced eval. acc: 0.728030303030303\n",
      "Confidence 0.22\n",
      "Balanced training. acc: 0.7588836055406243\n",
      "Balanced eval. acc: 0.7420454545454545\n",
      "Confidence 0.24\n",
      "Balanced training. acc: 0.7699441802770313\n",
      "Balanced eval. acc: 0.7553030303030303\n",
      "Confidence 0.26\n",
      "Balanced training. acc: 0.7822451933016332\n",
      "Balanced eval. acc: 0.7689393939393939\n",
      "Confidence 0.28\n",
      "Balanced training. acc: 0.7957866446144304\n",
      "Balanced eval. acc: 0.7803030303030303\n",
      "Confidence 0.3\n",
      "Balanced training. acc: 0.807674178209634\n",
      "Balanced eval. acc: 0.7916666666666666\n",
      "Confidence 0.32\n",
      "Balanced training. acc: 0.8162910895182964\n",
      "Balanced eval. acc: 0.7897727272727273\n",
      "Confidence 0.34\n",
      "Balanced training. acc: 0.8290055819722969\n",
      "Balanced eval. acc: 0.7981060606060606\n",
      "Confidence 0.36\n",
      "Balanced training. acc: 0.8374157535662601\n",
      "Balanced eval. acc: 0.8075757575757576\n",
      "Confidence 0.38\n",
      "Balanced training. acc: 0.8422451933016333\n",
      "Balanced eval. acc: 0.8196969696969697\n",
      "Confidence 0.4\n",
      "Balanced training. acc: 0.8451106057473641\n",
      "Balanced eval. acc: 0.8185606060606061\n",
      "Confidence 0.42\n",
      "Balanced training. acc: 0.8562745503411205\n",
      "Balanced eval. acc: 0.8162878787878788\n",
      "Confidence 0.44\n",
      "Balanced training. acc: 0.8606905106470952\n",
      "Balanced eval. acc: 0.8257575757575757\n",
      "Confidence 0.46\n",
      "Balanced training. acc: 0.8639694025222245\n",
      "Balanced eval. acc: 0.8333333333333333\n",
      "Confidence 0.48\n",
      "Balanced training. acc: 0.8558197229687823\n",
      "Balanced eval. acc: 0.8454545454545455\n",
      "Confidence 0.5\n",
      "Balanced training. acc: 0.8555178829853214\n",
      "Balanced eval. acc: 0.8507575757575758\n",
      "Confidence 0.52\n",
      "Balanced training. acc: 0.8568327475708084\n",
      "Balanced eval. acc: 0.8587121212121211\n",
      "Confidence 0.54\n",
      "Balanced training. acc: 0.8563241678726483\n",
      "Balanced eval. acc: 0.8348484848484848\n",
      "Confidence 0.56\n",
      "Balanced training. acc: 0.8520281166011991\n",
      "Balanced eval. acc: 0.8291666666666666\n",
      "Confidence 0.58\n",
      "Balanced training. acc: 0.8511060574736407\n",
      "Balanced eval. acc: 0.8011363636363636\n",
      "Confidence 0.6\n",
      "Balanced training. acc: 0.8513872234856317\n",
      "Balanced eval. acc: 0.8087121212121212\n",
      "Confidence 0.62\n",
      "Balanced training. acc: 0.837175935497209\n",
      "Balanced eval. acc: 0.781060606060606\n",
      "Confidence 0.64\n",
      "Balanced training. acc: 0.8283688236510234\n",
      "Balanced eval. acc: 0.7625\n",
      "Confidence 0.66\n",
      "Balanced training. acc: 0.8051726276617738\n",
      "Balanced eval. acc: 0.768939393939394\n",
      "Confidence 0.68\n",
      "Balanced training. acc: 0.7709613396733512\n",
      "Balanced eval. acc: 0.7511363636363636\n",
      "Confidence 0.7000000000000001\n",
      "Balanced training. acc: 0.7470415546826545\n",
      "Balanced eval. acc: 0.7431818181818182\n",
      "Confidence 0.72\n",
      "Balanced training. acc: 0.7247384742609055\n",
      "Balanced eval. acc: 0.7458333333333333\n",
      "Confidence 0.74\n",
      "Balanced training. acc: 0.6988546619805664\n",
      "Balanced eval. acc: 0.7053030303030303\n",
      "Confidence 0.76\n",
      "Balanced training. acc: 0.6783750258424643\n",
      "Balanced eval. acc: 0.6965909090909091\n",
      "Confidence 0.78\n",
      "Balanced training. acc: 0.6602356832747571\n",
      "Balanced eval. acc: 0.6973484848484849\n",
      "Confidence 0.8\n",
      "Balanced training. acc: 0.6362786851354145\n",
      "Balanced eval. acc: 0.6420454545454546\n",
      "Confidence 0.8200000000000001\n",
      "Balanced training. acc: 0.6054703328509407\n",
      "Balanced eval. acc: 0.6310606060606061\n",
      "Confidence 0.84\n",
      "Balanced training. acc: 0.5921149472813727\n",
      "Balanced eval. acc: 0.6314393939393939\n",
      "Confidence 0.86\n",
      "Balanced training. acc: 0.5637502584246433\n",
      "Balanced eval. acc: 0.5977272727272728\n",
      "Confidence 0.88\n",
      "Balanced training. acc: 0.5641637378540417\n",
      "Balanced eval. acc: 0.5867424242424243\n",
      "Confidence 0.9\n",
      "Balanced training. acc: 0.5644738474260905\n",
      "Balanced eval. acc: 0.5761363636363637\n",
      "Confidence 0.92\n",
      "Balanced training. acc: 0.5534587554269175\n",
      "Balanced eval. acc: 0.565530303030303\n",
      "Confidence 0.9400000000000001\n",
      "Balanced training. acc: 0.5507049824271242\n",
      "Balanced eval. acc: 0.565530303030303\n",
      "Confidence 0.96\n",
      "Balanced training. acc: 0.547951209427331\n",
      "Balanced eval. acc: 0.5659090909090909\n",
      "Confidence 0.98\n",
      "Balanced training. acc: 0.527951209427331\n",
      "Balanced eval. acc: 0.5662878787878788\n",
      "max_acc 0.8587121212121211, best_conf0.52\n"
     ]
    }
   ],
   "source": [
    "def output_parameters(confidence, grid, training_features, training_targets,evaluation_features, evaluation_targets, max_acc,best_conf, log = False ):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, roc_curve, recall_score, make_scorer\n",
    "\n",
    "    # Use all training data to calculate confusion matrix for training data\n",
    "    training_estimates_ = grid.predict(training_features)\n",
    "    training_estimates = np.where(training_estimates_ > confidence, 1, 0)\n",
    "\n",
    "    training_accuracy = balanced_accuracy_score(training_targets, training_estimates)\n",
    "    training_confusion_matrix = confusion_matrix(training_targets, training_estimates)\n",
    "\n",
    "    # Use model to estimate manually labeled evaluation Tweets\n",
    "    evaluation_estimates_ = grid.predict(evaluation_features)\n",
    "\n",
    "    evaluation_estimates = np.where(evaluation_estimates_ > confidence, 1, 0)\n",
    "\n",
    "    evaluation_accuracy = balanced_accuracy_score(evaluation_targets, evaluation_estimates)\n",
    "    evaluation_confusion_matrix = confusion_matrix(evaluation_targets, evaluation_estimates)\n",
    "    \n",
    "    # evaluation_confusion_matrix = confusion_matrix_to_string(evaluation_confusion_matrix)\n",
    "    if max_acc <= evaluation_accuracy:\n",
    "        max_acc = evaluation_accuracy\n",
    "        best_conf = confidence\n",
    "\n",
    "    print(f\"Confidence {confidence}\")\n",
    "    if log:\n",
    "        print(training_estimates_)\n",
    "        print(\"\\n\\n\\n\\n\\n\\n_________________________________________________\")\n",
    "        print(training_estimates)\n",
    "        print(training_confusion_matrix)\n",
    "        print(evaluation_estimates_[:5])\n",
    "        print(evaluation_estimates[:5])\n",
    "        print(evaluation_confusion_matrix)\n",
    "\n",
    "    print(f\"Balanced training. acc: {training_accuracy}\")\n",
    "\n",
    "    print(f\"Balanced eval. acc: {evaluation_accuracy}\")\n",
    "\n",
    "    # summarize results\n",
    "\n",
    "    if log:\n",
    "        print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "        means = grid.cv_results_['mean_test_score']\n",
    "        stds = grid.cv_results_['std_test_score']\n",
    "        params = grid.cv_results_['params']\n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "        print(\"Training evaluation\")\n",
    "\n",
    "        y_pred_train_ = grid.predict(training_features)\n",
    "        print(y_pred_train_)\n",
    "        y_pred_train = np.where(y_pred_train_ > confidence, 1, 0)\n",
    "        print(classification_report(training_targets, y_pred_train))\n",
    "        train_conf_matrix = confusion_matrix(training_targets, y_pred_train)\n",
    "\n",
    "        print(\"Validation/evaluation\")\n",
    "        y_pred_test_ = grid.predict(evaluation_features) # prediction on our test set\n",
    "\n",
    "        print(y_pred_test_)\n",
    "        y_pred_test = np.where(y_pred_test_ > confidence, 1, 0)\n",
    "\n",
    "        print(classification_report(evaluation_targets, y_pred_test))\n",
    "        test_conf_matrix = confusion_matrix(evaluation_targets, y_pred_test)\n",
    "        print(test_conf_matrix)\n",
    "    return max_acc, best_conf\n",
    "\n",
    "confidences =  numpy.arange(0, 1,0.02)\n",
    "max_acc = 0\n",
    "best_conf = 0\n",
    "log = False\n",
    "\n",
    "for confidence in confidences:\n",
    "    max_acc,best_conf = output_parameters(confidence, grid, training_features, training_targets,evaluation_features, evaluation_targets, max_acc,best_conf, log = False )\n",
    "\n",
    "print(f\"max_acc {max_acc}, best_conf{best_conf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence 0.52\n",
      "[0.8122701  0.45338953 0.08439553 ... 0.7451928  0.7023279  0.58131725]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_________________________________________________\n",
      "[1 0 0 ... 1 1 1]\n",
      "[[4143  694]\n",
      " [  25  150]]\n",
      "[0.         0.5057441  0.38077897 0.06377809 0.10622675]\n",
      "[0 0 0 0 0]\n",
      "[[1127  193]\n",
      " [   6   38]]\n",
      "Balanced training. acc: 0.8568327475708084\n",
      "Balanced eval. acc: 0.8587121212121211\n",
      "Best: 0.572992 using {'clf__dropout': 0.2, 'clf__epochs': 10, 'clf__kernel_initializer': 'uniform', 'clf__layers_n': 64, 'clf__metrics': 'accuracy', 'clf__optimizer': 'adam', 'selection__k': 30, 'selection__score_func': <function chi2 at 0x0000017B1577CB80>}\n",
      "0.572992 (0.469592) with: {'clf__dropout': 0.2, 'clf__epochs': 10, 'clf__kernel_initializer': 'uniform', 'clf__layers_n': 64, 'clf__metrics': 'accuracy', 'clf__optimizer': 'adam', 'selection__k': 30, 'selection__score_func': <function chi2 at 0x0000017B1577CB80>}\n",
      "Training evaluation\n",
      "[0.8122701  0.45338953 0.08439553 ... 0.7451928  0.7023279  0.58131725]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92      4837\n",
      "           1       0.18      0.86      0.29       175\n",
      "\n",
      "    accuracy                           0.86      5012\n",
      "   macro avg       0.59      0.86      0.61      5012\n",
      "weighted avg       0.97      0.86      0.90      5012\n",
      "\n",
      "Validation/evaluation\n",
      "[0.         0.5057441  0.38077897 ... 0.60366803 0.         0.6875018 ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92      1320\n",
      "           1       0.16      0.86      0.28        44\n",
      "\n",
      "    accuracy                           0.85      1364\n",
      "   macro avg       0.58      0.86      0.60      1364\n",
      "weighted avg       0.97      0.85      0.90      1364\n",
      "\n",
      "[[1127  193]\n",
      " [   6   38]]\n"
     ]
    }
   ],
   "source": [
    "confidence = best_conf\n",
    "max_acc,best_conf = output_parameters(best_conf, grid, training_features, training_targets,evaluation_features, evaluation_targets, max_acc,best_conf, log = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS_VALIDATION_CV: 5\n",
      "EVALUATION_FILES: ['../data/original/evaluation_dataset.csv', '..\\\\data\\\\preprocessed\\\\difference_preprocessed_evaluation_dataset.csv', '..\\\\data\\\\preprocessed\\\\ole_RemoveOutliers_and_preprocessing_pipe_evaluation_dataset.csv']\n",
      "FILE_N: 2\n",
      "HYPER_PARAMETER_ALPHA: [0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06]\n",
      "HYPER_PARAMETER_BATCH_NUMBER: [20, 50, 80, 110, 140]\n",
      "HYPER_PARAMETER_CLASIFIER_METRICS: ['accuracy']\n",
      "HYPER_PARAMETER_CLASIFIER_OPTIMIZERS: ['rmsprop', 'adam', 'adagrad']\n",
      "HYPER_PARAMETER_DROPUT: [0, 0.1, 0.2, 0.3]\n",
      "HYPER_PARAMETER_EPOCHS: [10, 20, 40, 60]\n",
      "HYPER_PARAMETER_HIDDEN_LAYER_SIZES: [(100,)]\n",
      "HYPER_PARAMETER_KERNEL_INITIALIZER: ['glorot_uniform', 'normal', 'uniform']\n",
      "HYPER_PARAMETER_K_FEATURES: [30, 40, 50, 60, 70, 80]\n",
      "HYPER_PARAMETER_LAYERS_N: [20, 30, 40, 50, 60, 70, 80]\n",
      "HYPER_PARAMETER_OPTIMIZATION_BETA: 2\n",
      "HYPER_PARAMETER_OPTIMIZATION_CV: 5\n",
      "HYPER_PARAMETER_OPTIMIZATION_SCORING: accuracy\n",
      "HYPER_PARAMETER_SCORE_FUNC: [<function chi2 at 0x0000017B1577CB80>, <function f_classif at 0x0000017B1577CA60>]\n",
      "INPUT_EVALUATION_FILE: ..\\data\\preprocessed\\ole_RemoveOutliers_and_preprocessing_pipe_evaluation_dataset.csv\n",
      "INPUT_TRAINING_FILE: ..\\data\\preprocessed\\ole_RemoveOutliers_and_preprocessing_pipe_training_dataset.csv\n",
      "LABELS: ['Operational', 'Bankrupt']\n",
      "METHOD_NAME: keras_tensorflow_resnet\n",
      "OUTPUT_MODEL: ../data/models/keras_tensorflow_resnet_keras_model_19_05_2022-14_10_10.pkl\n",
      "OUTPUT_MODEL_ARCHITECTURE_RESULTS: ../img/keras_tensorflow_resnet_model_19_05_2022-14_10_10.png\n",
      "OUTPUT_MODEL_FOLDER: ../data/models/keras_models\n",
      "OUTPUT_RESULTS: ../data/results/keras_tensorflow_resnet_model_19_05_2022-14_10_10.txt\n",
      "PUNISHMENT_FOR_FALSE_BANKRUPT_PREDICTION: 100\n",
      "RANDOM_SEED: 42\n",
      "TIMESTAMP: 19_05_2022-14_10_10\n",
      "TRAINING_FILES: ['../data/original/training_dataset.csv', '..\\\\data\\\\preprocessed\\\\difference_preprocessed_training_dataset.csv', '..\\\\data\\\\preprocessed\\\\ole_RemoveOutliers_and_preprocessing_pipe_training_dataset.csv']\n",
      "best_conf: 0.52\n",
      "confidence: 0.52\n",
      "max_acc: 0.8587121212121211\n",
      "param_grid: {'clf__optimizer': ['adam'], 'clf__epochs': [10], 'clf__dropout': [0.2], 'clf__layers_n': [64], 'clf__kernel_initializer': ['uniform'], 'clf__metrics': ['accuracy'], 'selection__k': [30], 'selection__score_func': [<function chi2 at 0x0000017B1577CB80>]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_variables = dir()\n",
    "\n",
    "results_string = \"\"\n",
    "\n",
    "for variable in current_variables:\n",
    "    # Skip environment variables and their container variable\n",
    "    # Ignore also underscore variables\n",
    "    if variable in environment_variables or variable == \"environment_variables\" or variable.startswith(\"_\"):\n",
    "        continue\n",
    "\n",
    "    # Get variables value\n",
    "    variable_value = globals()[variable]\n",
    "\n",
    "    # If variable is numerical or string, append it to results\n",
    "    if type(variable_value) is str or type(variable_value) is int or \\\n",
    "        type(variable_value) is float or type(variable_value) is list or \\\n",
    "        type(variable_value) is numpy.float64 or type(variable_value) is dict:\n",
    "        results_string += f\"{variable}: {variable_value}\\n\"\n",
    "\n",
    "# Print results to screen\n",
    "print(results_string)\n",
    "\n",
    "# Save results to file\n",
    "with open(OUTPUT_RESULTS, \"w\") as file:\n",
    "    file.write(results_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEIVID~1\\AppData\\Local\\Temp/ipykernel_12296/3292626468.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUTPUT_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip install import weakref\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open(OUTPUT_MODEL, \"wb\") as handle:\n",
    "    joblib.dump(grid, OUTPUT_MODEL)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d2cc45be89154c81e316490ce08739f90b548aca5356e8d287f3c1f6b5a8c71"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
